#  "Deep Learning" Goodfellow et al. (2016), Chapter 1

I just ordered a copy of "Deep Learning," and I wanted to do a chapter-by-chapter review. It is actually available [here](http://www.deeplearningbook.org/) for free. I will just go over the parts that I thought are interesting and digest each chapter into a 5-10 minute read. I will pitch the ideas in this book for someone who is a beginner at machine learning.

The funny thing about artificial intelligence is that it is actually a fairly old idea. According to this book, the first ideas about deep learning came about in the 1940's. I took a class with Professor [Jay McClelland](https://en.wikipedia.org/wiki/James_McClelland_(psychologist)), and I was shocked when I found out how much of an impact he and his colleagues had on deep learning twenty or thirty years ago. He told our class that when they first started doing psychology research with perceptrons and parallel distributed systems, many of the established researchers were very skeptical of the application of neural networks in neuroscience. Sure enough, we often see neural networks conflated with how the brain actually works, but this book is careful not to make that leap.

> Today, neuroscience is regarded as an important source of inspiration for deep learning researchers, but *it is no longer the predominant guide for the field*. The main reason for the diminished role of neuroscience in deep learning research today is that *we simply do not have enough information about the brain* to use it as a guide. (pg 14-15)

In short, we have to be cautious about thinking that AI informs neuroscience or vice versa.

This book identifies **three waves of AI**: the birth of simple learning algorithms in the 1940s, the connectionism movement in the 1980s, and the deep learning movement in the late 2000s. Interest in AI has come and gone with money, hype, and processing power, but right now is the most exciting time to be in AI.

> Deep learning was often regarded as being more of an art than a technology and something that only an expert could us, until recently. It is true that some skill is required to get good performance from a deep learning algorithm. Fortunately, *the amount of skill required reduces as the amount of training data increases.* (pg 18)

The point is that the barrier of entry for AI is surprisingly low, and the surplus of data could make up for one's lack of intuition about deep learning. If you follow along with TensorFlow or PyTorch tutorials, it is actually quite easy to use the frameworks, and free datasets can be found anywhere. I took Andrew Ng's CS229 earlier this year, and it was kind of funny to think about how much theory we studied and how little theory we used in our final project.

This chapter name-drops a few deep learning concepts, but I assume that it will go into more detail later on. Perhaps for the casual reader, I will try to explain what exactly deep learning is, and how it fits into our understanding of artificial intelligence.

## Personal Takes

I like the way that the book presents deep learning. There's a venn diagram that looks a lot like this:

<img class="article-img" src="/assets/chapter1/venn.PNG"/>

I would say that Artificial Intelligence can be broadly construed as any system that can autonomously perform any sort of cognitively demanding task. Let us define an AI system as a **model**. What makes a model intelligent is perhaps an epistemic question, but for now, I would even include something that is completely programmed. For something that is completely programmed, we can say that we have high control over it.

### The Hierarchy of Learning Models

The difference between machine learning, representation learning, and deep learning is really about how much control we hand over to the model. In classical machine learning, we would perform **feature extraction** on an input; in layman's terms, we have control over how the model interprets an input. In representation learning, we force the model to do this step itself; the model has to learn *how* to interpret an input. In deep learning, we give the model a lot more flexibility by introducing **layers**. The layers are essentially mathematical functions, and they manipulate the input with matrix transformations, non-linear functions, and such. We will discuss these ideas in greater detail in later chapters, so the main takeaway is that deep learning is representation learning with more complicated models.
